import streamlit as st
import pandas as pd
import numpy as np
import io
import hashlib
import json
from typing import List, Tuple, Dict, Any

from sentence_transformers import SentenceTransformer, util, InputExample, losses
from torch.utils.data import DataLoader
import torch

# --------------------
# –£—Ç–∏–ª–∏—Ç—ã
# --------------------

def preprocess_text(t: Any) -> str:
    if pd.isna(t):
        return ""
    return " ".join(str(t).lower().strip().split())


def file_md5(b: bytes) -> str:
    return hashlib.md5(b).hexdigest()


def read_uploaded_file_bytes(uploaded) -> Tuple[pd.DataFrame, str]:
    """–ü—Ä–æ—á–∏—Ç–∞—Ç—å CSV –∏–ª–∏ Excel –∏–∑ streamlit uploader –∏ –≤–µ—Ä–Ω—É—Ç—å DataFrame + md5 —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ."""
    raw = uploaded.read()
    h = file_md5(raw)
    try:
        df = pd.read_csv(io.BytesIO(raw))
    except Exception:
        try:
            df = pd.read_excel(io.BytesIO(raw))
        except Exception as e:
            raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å CSV –∏–ª–∏ Excel. –û—à–∏–±–∫–∞: " + str(e))
    return df, h


def parse_topics_field(val) -> List[str]:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–ª–µ topics –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: NaN -> [], JSON-—Å–ø–∏—Å–æ–∫ -> parsed, —Å—Ç—Ä–æ–∫–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º , ; | -> split
    """
    if pd.isna(val):
        return []
    if isinstance(val, list):
        return [str(x).strip() for x in val if str(x).strip()]
    s = str(val).strip()
    # –ü–æ–ø—Ä–æ–±—É–µ–º –∫–∞–∫ JSON
    if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
        try:
            parsed = json.loads(s)
            if isinstance(parsed, list):
                return [str(x).strip() for x in parsed if str(x).strip()]
        except Exception:
            pass
    # –ò–Ω–∞—á–µ split –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
    for sep in [";", "|", ","]:
        if sep in s:
            parts = [p.strip() for p in s.split(sep) if p.strip()]
            return parts
    # –æ–¥–∏–Ω–æ—á–Ω–∞—è —Ç–µ–º–∞
    return [s] if s else []


@st.cache_resource(show_spinner=False)
def load_model_cached(model_path: str):
    return SentenceTransformer(model_path)


def encode_texts_in_batches(model, texts: List[str], batch_size: int = 64) -> np.ndarray:
    """–≠–Ω–∫–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º numpy array (CPU).
    –ò—Å–ø–æ–ª—å–∑—É–µ–º SentenceTransformer.encode —Å convert_to_numpy=True.
    """
    if not texts:
        return np.array([])
    embs = model.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)
    return np.asarray(embs)


def cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None:
        return 0.0
    if np.all(a == 0) or np.all(b == 0):
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


def jaccard_tokens(a: str, b: str) -> float:
    sa = set([t for t in a.split() if t])
    sb = set([t for t in b.split() if t])
    if not sa and not sb:
        return 0.0
    inter = sa & sb
    union = sa | sb
    return len(inter) / len(union) if union else 0.0


def dedupe_pairs(pairs: List[Tuple[int, int, float, float]]) -> List[Tuple[int, int, float, float]]:
    """–£–±–∏—Ä–∞–µ–º –∑–µ—Ä–∫–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (i,j) –∏ (j,i) ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –≤—Å—Ç—Ä–µ—á–Ω—É—é –ø–∞—Ä—É."""
    best = {}
    for i, j, mscore, lex in pairs:
        key = tuple(sorted((i, j)))
        if key not in best:
            best[key] = (i, j, mscore, lex)
    return list(best.values())


# --------------------
# UI –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
# --------------------
st.set_page_config(page_title="Synonym Checker (with topics)", layout="wide")
st.title("üîé Synonym Checker ‚Äî –†—É—á–Ω–æ–π –∏ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º—ã (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π topics)")

# Sidebar settings
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –æ–±—É—á–µ–Ω–∏—è")
model_path = st.sidebar.text_input("–ü—É—Ç—å –∏–ª–∏ –∏–º—è –º–æ–¥–µ–ª–∏ (SentenceTransformer)", value="fine_tuned_model")
output_model_path = st.sidebar.text_input("–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–ø–∞–ø–∫–∞)", value="fine_tuned_model_updated")
batch_size = st.sidebar.number_input("Batch size –¥–ª—è —ç–Ω–∫–æ–¥–∏–Ω–≥–∞", min_value=8, max_value=1024, value=64, step=8)
train_batch_size = st.sidebar.number_input("Batch size –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è", min_value=4, max_value=64, value=8, step=1)
train_epochs = st.sidebar.number_input("–≠–ø–æ—Ö–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è", min_value=1, max_value=10, value=1, step=1)
warmup_steps = st.sidebar.number_input("Warmup steps (train)", min_value=0, max_value=1000, value=10, step=1)
DEFAULT_MAX_ROWS = st.sidebar.number_input("–ú–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–æ–∫ (auto mode)", min_value=100, max_value=20000, value=5000, step=100)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
try:
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å..."):
        model = load_model_cached(model_path)
    st.sidebar.success("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
except Exception as e:
    st.sidebar.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
    st.stop()

# session_state cache –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø–æ —Ö—ç—à—É —Ñ–∞–π–ª–∞)
if "emb_cache" not in st.session_state:
    st.session_state["emb_cache"] = {}

# –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏
tab_manual, tab_auto = st.tabs(["üîß –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º (CSV/Excel –ø–∞—Ä)", "ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º (–∫–æ—Ä–ø—É—Å —Ñ—Ä–∞–∑)"])

# ----------------------
# –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º
# ----------------------
with tab_manual:
    st.header("üîß –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç–µ CSV/XLSX —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: phrase, synonym (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: topics)")
    uploaded = st.file_uploader("–§–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ (CSV/XLSX)", type=["csv", "xlsx", "xls"], key="manual_upl")

    if uploaded is not None:
        try:
            df_pairs, file_hash = read_uploaded_file_bytes(uploaded)
        except Exception as e:
            st.error(str(e))
            st.stop()

        # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ (—Ö—Ä–∞–Ω–∏–º mapping –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–º–µ–Ω–∏)
        cols_lower = {c.lower(): c for c in df_pairs.columns}
        if "phrase" not in cols_lower or "synonym" not in cols_lower:
            st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: phrase –∏ synonym (—Ä–µ–≥–∏—Å—Ç—Ä –±—É–∫–≤ –Ω–µ –≤–∞–∂–µ–Ω)")
            st.stop()

        phrase_col = cols_lower["phrase"]
        synonym_col = cols_lower["synonym"]
        topics_col = cols_lower.get("topics")  # –º–æ–∂–µ—Ç –±—ã—Ç—å None

        # preprocess
        df_pairs = df_pairs.copy()
        df_pairs["phrase_proc"] = df_pairs[phrase_col].apply(preprocess_text)
        df_pairs["syn_proc"] = df_pairs[synonym_col].apply(preprocess_text)
        if topics_col:
            df_pairs["topics"] = df_pairs[topics_col].apply(parse_topics_field)
            # —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã
            all_topics = sorted({t for ts in df_pairs["topics"] for t in ts})
            selected_topics = st.multiselect("–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º (manual)", all_topics)
            if selected_topics:
                df_pairs = df_pairs[df_pairs["topics"].apply(lambda ts: bool(set(ts) & set(selected_topics)))].reset_index(drop=True)
        else:
            df_pairs["topics"] = [[] for _ in range(len(df_pairs))]
            selected_topics = []

        if df_pairs.empty:
            st.warning("–ù–µ—Ç –ø–∞—Ä –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–∞–º (–∏–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç).")
            st.stop()

        # —ç–Ω–∫–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        unique_texts = list({t for t in pd.concat([df_pairs["phrase_proc"], df_pairs["syn_proc"]]) if pd.notna(t) and t})
        st.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –¥–ª—è —ç–Ω–∫–æ–¥–∏–Ω–≥–∞: {len(unique_texts)}")

        cache_key = f"manual_{file_hash}_{model_path}_{batch_size}"
        if cache_key in st.session_state["emb_cache"]:
            emb_map = st.session_state["emb_cache"][cache_key]["map"]
            st.success("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
        else:
            with st.spinner("–≠–Ω–∫–æ–¥–∏–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã..."):
                emb_arr = encode_texts_in_batches(model, unique_texts, batch_size=int(batch_size))
                emb_map = {text: emb for text, emb in zip(unique_texts, emb_arr)}
            st.session_state["emb_cache"][cache_key] = {"map": emb_map}
            st.success("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã")

        # –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞
        results = []
        for _, row in df_pairs.iterrows():
            a_proc = row["phrase_proc"]
            b_proc = row["syn_proc"]
            emb_a = emb_map.get(a_proc)
            emb_b = emb_map.get(b_proc)
            score = cos_sim(emb_a, emb_b) if emb_a is not None and emb_b is not None else None
            results.append({
                "phrase": row[phrase_col],
                "synonym": row[synonym_col],
                "topics": row["topics"],
                "score": score
            })

        res_df = pd.DataFrame(results).sort_values(by="score", ascending=True, na_position="last")
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã (manual)")
        st.dataframe(res_df, use_container_width=True)

        cut = st.slider("–ü–æ—Ä–æ–≥: —Å—á–∏—Ç–∞—Ç—å –ø–∞—Ä—É —Å–ª–∞–±–æ–π, –µ—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ < ", 0.0, 1.0, 0.75, 0.01, key="manual_thresh")
        bad_df = res_df[res_df["score"].notna() & (res_df["score"] < float(cut))].reset_index(drop=True)
        st.markdown(f"–ù–∞–π–¥–µ–Ω–æ —Å–ª–∞–±—ã—Ö –ø–∞—Ä: **{len(bad_df)}** (score &lt; {cut})")

        # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not res_df.empty:
            csv_buf = io.StringIO()
            res_df.to_csv(csv_buf, index=False, encoding="utf-8-sig")
            st.download_button("üíæ –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)", csv_buf.getvalue(), file_name="synonym_check_results_manual.csv", mime="text/csv")

        # train_data.json –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è (–Ω–∞ —Å–ª–∞–±—ã—Ö –ø–∞—Ä–∞—Ö)
        train_data = []
        for _, r in bad_df.iterrows():
            train_data.append({"texts": [r["phrase"], r["synonym"]], "label": 1.0, "topics": r.get("topics", [])})

        json_buf = io.StringIO()
        json.dump(train_data, json_buf, ensure_ascii=False, indent=2)
        if train_data:
            st.download_button("üíæ –°–∫–∞—á–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è (JSON)", json_buf.getvalue(), file_name="train_data_manual.json", mime="application/json")
        else:
            st.info("–ù–µ—Ç —Å–ª–∞–±—ã—Ö –ø–∞—Ä ‚Äî –Ω–∏—á–µ–≥–æ –Ω–µ –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è.")

        # –î–æ–æ–±—É—á–µ–Ω–∏–µ
        if st.button("üöÄ –î–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª–∞–±—ã—Ö –ø–∞—Ä–∞—Ö (manual)"):
            if not train_data:
                st.warning("–ù–µ—Ç —Å–ª–∞–±—ã—Ö –ø–∞—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è.")
            else:
                with st.spinner("–î–æ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å..."):
                    train_examples = [InputExample(texts=[t[0], t[1]], label=1.0) for t in [(x["texts"][0], x["texts"][1]) for x in train_data]]
                    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=int(train_batch_size))
                    train_loss = losses.CosineSimilarityLoss(model)
                    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=int(train_epochs), warmup_steps=int(warmup_steps))
                    try:
                        model.save(output_model_path)
                        st.success(f"–ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_model_path}")
                    except Exception as e:
                        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

# ----------------------
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º
# ----------------------
with tab_auto:
    st.header("ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º ‚Äî –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —Ñ—Ä–∞–∑–∞–º–∏ (CSV/XLSX)")
    st.markdown("–ú—ã –∏—â–µ–º –ª–µ–∫—Å–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä—ã (Jaccard —Ç–æ–∫–µ–Ω–æ–≤), –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç –ù–ï –ø–æ—Ö–æ–∂–∏–º–∏ ‚Äî –ª—É—á—à–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
    uploaded_auto = st.file_uploader("CSV/Excel —Å –∫–æ–ª–æ–Ω–∫–æ–π 'phrase' (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: topics, comment)", type=["csv", "xlsx", "xls"], key="auto_upl")

    if uploaded_auto is not None:
        try:
            df_all, file_hash = read_uploaded_file_bytes(uploaded_auto)
        except Exception as e:
            st.error(str(e))
            st.stop()

        cols_lower = {c.lower(): c for c in df_all.columns}
        if "phrase" not in cols_lower:
            st.error("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É 'phrase' (—Ä–µ–≥–∏—Å—Ç—Ä –Ω–µ –≤–∞–∂–µ–Ω).")
            st.stop()

        phrase_col = cols_lower["phrase"]
        topics_col = cols_lower.get("topics")

        df_all = df_all.copy()
        df_all["phrase_proc"] = df_all[phrase_col].apply(preprocess_text)
        if topics_col:
            df_all["topics"] = df_all[topics_col].apply(parse_topics_field)
            all_topics = sorted({t for ts in df_all["topics"] for t in ts})
            selected_topics = st.multiselect("–§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–º–∞—Ç–∏–∫–∞–º (auto)", all_topics)
            if selected_topics:
                df_all = df_all[df_all["topics"].apply(lambda ts: bool(set(ts) & set(selected_topics)))].reset_index(drop=True)
        else:
            df_all["topics"] = [[] for _ in range(len(df_all))]
            selected_topics = []

        texts = df_all["phrase_proc"].fillna("").tolist()
        n = len(texts)
        st.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ—Ä–∞–∑: {n}")

        if n == 0:
            st.warning("–ù–µ—Ç —Ñ—Ä–∞–∑ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–∞–º –ø—É—Å—Ç–æ).")
            st.stop()

        if n > int(DEFAULT_MAX_ROWS):
            st.warning(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({n} —Å—Ç—Ä–æ–∫). –û–±—Ä–∞–±–æ—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ {int(DEFAULT_MAX_ROWS)} —Å—Ç—Ä–æ–∫.")
            df_all = df_all.head(int(DEFAULT_MAX_ROWS))
            texts = df_all["phrase_proc"].tolist()
            n = len(texts)

        st.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
        col1, col2, col3 = st.columns(3)
        top_k = int(col1.number_input("Top-K —Å–æ—Å–µ–¥–µ–π (–Ω–∞ —Ñ—Ä–∞–∑—É)", min_value=1, max_value=50, value=5, step=1))
        lexical_thresh = float(col2.slider("–õ–µ–∫—Å–∏—á. —Å—Ö–æ–∂–µ—Å—Ç—å (Jaccard) >= ", 0.0, 1.0, 0.5, 0.05))
        model_thresh = float(col3.slider("–ú–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç —Å—Ö–æ–∂–∏–º–∏ –µ—Å–ª–∏ score >= ", 0.0, 1.0, 0.75, 0.01))

        cache_key = f"auto_{file_hash}_{model_path}_{batch_size}"
        if cache_key in st.session_state["emb_cache"]:
            embs = st.session_state["emb_cache"][cache_key]["emb"]
            st.success("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ñ–∞–π–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫—ç—à–∞")
        else:
            with st.spinner("–≠–Ω–∫–æ–¥–∏–º –≤—Å–µ —Ñ—Ä–∞–∑—ã (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è)..."):
                embs = encode_texts_in_batches(model, texts, batch_size=int(batch_size))
                st.session_state["emb_cache"][cache_key] = {"emb": embs, "texts": texts, "df": df_all}
            st.success("–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã")

        # semantic_search (all-against-all)
        with st.spinner("–ò—â—É –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –∏ —Ñ–æ—Ä–º–∏—Ä—É—é –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤..."):
            corpus_emb = torch.from_numpy(embs)
            query_emb = corpus_emb
            hits = util.semantic_search(query_embedding=query_emb, corpus_embeddings=corpus_emb, top_k=top_k)

            candidate_pairs: List[Tuple[int, int, float, float]] = []
            for i, neighbors in enumerate(hits):
                for hit in neighbors:
                    j = int(hit["corpus_id"])
                    if i == j:
                        continue
                    mscore = float(hit["score"])
                    lex = jaccard_tokens(texts[i], texts[j])
                    if lex >= float(lexical_thresh) and mscore < float(model_thresh):
                        candidate_pairs.append((i, j, mscore, lex))

            candidate_pairs = dedupe_pairs(candidate_pairs)
            candidate_pairs = sorted(candidate_pairs, key=lambda x: (x[2], -x[3]))

        st.markdown(f"–ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã—Ö –ø–∞—Ä: **{len(candidate_pairs)}** (–ª–µ–∫—Å–∏—á. —Å—Ö–æ–∂–µ—Å—Ç—å ‚â• {lexical_thresh} –∏ –º–æ–¥–µ–ª—å score &lt; {model_thresh})")

        if not candidate_pairs:
            st.info("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ—Ä–æ–≥–∏.")
        else:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º —Ç–∞–±–ª–∏—Ü—É –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∫–∞–∑–∞
            display_rows = []
            for idx, (i, j, mscore, lex) in enumerate(candidate_pairs):
                pa_orig = df_all.iloc[i][phrase_col]
                pb_orig = df_all.iloc[j][phrase_col]
                ta = df_all.iloc[i]["topics"] if "topics" in df_all.columns else []
                tb = df_all.iloc[j]["topics"] if "topics" in df_all.columns else []
                display_rows.append({
                    "idx": idx,
                    "phrase_a": pa_orig,
                    "phrase_b": pb_orig,
                    "model_score": mscore,
                    "lex_jaccard": lex,
                    "topics_a": ta,
                    "topics_b": tb,
                })

            cand_df = pd.DataFrame(display_rows)
            st.subheader("–ö–∞–Ω–¥–∏–¥–∞—Ç—ã (–ø—Ä–∏–º–µ—Ä) ‚Äî —Ç–∞–±–ª–∏—Ü–∞")
            st.dataframe(cand_df[["idx", "phrase_a", "phrase_b", "model_score", "lex_jaccard", "topics_a", "topics_b"]], use_container_width=True)

            st.subheader("–û—Ç–º–µ—Ç—å—Ç–µ –ø–∞—Ä—ã –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –¥–∞—Ç–∞—Å–µ—Ç (train)")
            select_all = st.checkbox("–í—ã–±—Ä–∞—Ç—å –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã", value=True, key=f"auto_sel_all_{file_hash}")
            selected_indices = []
            # –í—ã–≤–æ–¥ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ —Å —á–µ–∫–±–æ–∫—Å–æ–º
            for row in display_rows:
                cols = st.columns([4, 4, 1, 1, 1])
                cols[0].write(row["phrase_a"])
                cols[1].write(row["phrase_b"])
                cols[2].write(f"{row['model_score']:.3f}")
                cols[3].write(f"{row['lex_jaccard']:.3f}")
                cols[4].write(", ".join(row['topics_a']) if row['topics_a'] else "")
                key = f"cand_{file_hash}_{row['idx']}"
                default = True if select_all else False
                sel = st.checkbox("", value=default, key=key)
                if sel:
                    selected_indices.append(row['idx'])

            st.markdown(f"–û—Ç–º–µ—á–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: **{len(selected_indices)}**")

            # –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å train_data.json
            if st.button("üìÇ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–∫–∞—á–∞—Ç—å train_data_auto.json"):
                chosen = []
                for idx in selected_indices:
                    i, j, mscore, lex = candidate_pairs[idx]
                    a = df_all.iloc[i][phrase_col]
                    b = df_all.iloc[j][phrase_col]
                    chosen.append({"texts": [a, b], "label": 1.0, "topics": df_all.iloc[i]["topics"]})
                if not chosen:
                    st.warning("–ù–µ –≤—ã–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –ø–∞—Ä—ã.")
                else:
                    json_buf = io.StringIO()
                    json.dump(chosen, json_buf, ensure_ascii=False, indent=2)
                    st.download_button("üíæ –°–∫–∞—á–∞—Ç—å train_data_auto.json", json_buf.getvalue(), file_name="train_data_auto.json", mime="application/json")
                    st.success(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {len(chosen)} –ø–∞—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è.")

            # –î–æ–æ–±—É—á–µ–Ω–∏–µ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º
            if st.button("üöÄ –î–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞—Ö (auto)"):
                chosen = []
                for idx in selected_indices:
                    i, j, mscore, lex = candidate_pairs[idx]
                    a = df_all.iloc[i][phrase_col]
                    b = df_all.iloc[j][phrase_col]
                    chosen.append({"texts": [a, b], "label": 1.0})
                if not chosen:
                    st.warning("–ù–µ—á–µ–≥–æ –¥–æ–æ–±—É—á–∞—Ç—å ‚Äî –Ω–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä.")
                else:
                    with st.spinner("–î–æ–æ–±—É—á–∞–µ–º..."):
                        train_examples = [InputExample(texts=item["texts"], label=item["label"]) for item in chosen]
                        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=int(train_batch_size))
                        train_loss = losses.CosineSimilarityLoss(model)
                        model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=int(train_epochs), warmup_steps=int(warmup_steps))
                        try:
                            model.save(output_model_path)
                            st.success(f"–ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_model_path}")
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")

# –ö–æ–Ω–µ—Ü
st.markdown("---")
st.caption("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏—â–µ—Ç –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤. \nManual ‚Äî –≤—ã –∑–∞–≥—Ä—É–∂–∞–µ—Ç–µ –ø–∞—Ä—ã. Auto ‚Äî –≤—ã –¥–∞—ë—Ç–µ –∫–æ—Ä–ø—É—Å —Ñ—Ä–∞–∑ –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ç–µ–º—ã; —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–π –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏ —Å–ª–∞–±–æ–π –º–æ–¥–µ–ª—å–Ω–æ–π —Å–≤—è–∑–∏.")
