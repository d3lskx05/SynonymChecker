import streamlit as st
import pandas as pd
import numpy as np
import io
import hashlib
import json
import tempfile
import os
import subprocess
from typing import List, Tuple, Dict, Any

from sentence_transformers import SentenceTransformer, util, InputExample, losses
from torch.utils.data import DataLoader
import torch
import altair as alt

# --------------------
# –£—Ç–∏–ª–∏—Ç—ã
# --------------------

def preprocess_text(t: Any) -> str:
    if pd.isna(t):
        return ""
    return " ".join(str(t).lower().strip().split())

def file_md5(b: bytes) -> str:
    return hashlib.md5(b).hexdigest()

def read_uploaded_file_bytes(uploaded) -> Tuple[pd.DataFrame, str]:
    """–ü—Ä–æ—á–∏—Ç–∞—Ç—å CSV –∏–ª–∏ Excel –∏–∑ streamlit uploader –∏ –≤–µ—Ä–Ω—É—Ç—å DataFrame + md5 —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ."""
    raw = uploaded.read()
    h = file_md5(raw)
    try:
        df = pd.read_csv(io.BytesIO(raw))
    except Exception:
        try:
            df = pd.read_excel(io.BytesIO(raw))
        except Exception as e:
            raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å CSV –∏–ª–∏ Excel. –û—à–∏–±–∫–∞: " + str(e))
    return df, h

def parse_topics_field(val) -> List[str]:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–ª–µ topics –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: NaN -> [], JSON-—Å–ø–∏—Å–æ–∫ -> parsed, —Å—Ç—Ä–æ–∫–∞ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º , ; | -> split
    """
    if pd.isna(val):
        return []
    if isinstance(val, list):
        return [str(x).strip() for x in val if str(x).strip()]
    s = str(val).strip()
    # –ü–æ–ø—Ä–æ–±—É–µ–º –∫–∞–∫ JSON
    if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
        try:
            parsed = json.loads(s)
            if isinstance(parsed, list):
                return [str(x).strip() for x in parsed if str(x).strip()]
        except Exception:
            pass
    # –ò–Ω–∞—á–µ split –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
    for sep in [";", "|", ","]:
        if sep in s:
            parts = [p.strip() for p in s.split(sep) if p.strip()]
            return parts
    # –æ–¥–∏–Ω–æ—á–Ω–∞—è —Ç–µ–º–∞
    return [s] if s else []

def cos_sim(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None:
        return 0.0
    if np.all(a == 0) or np.all(b == 0):
        return 0.0
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def jaccard_tokens(a: str, b: str) -> float:
    sa = set([t for t in a.split() if t])
    sb = set([t for t in b.split() if t])
    if not sa and not sb:
        return 0.0
    inter = sa & sb
    union = sa | sb
    return len(inter) / len(union) if union else 0.0

def dedupe_pairs(pairs: List[Tuple[int, int, float, float]]) -> List[Tuple[int, int, float, float]]:
    """–£–±–∏—Ä–∞–µ–º –∑–µ—Ä–∫–∞–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (i,j) –∏ (j,i) ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –≤—Å—Ç—Ä–µ—á–Ω—É—é –ø–∞—Ä—É."""
    best = {}
    for i, j, mscore, lex in pairs:
        key = tuple(sorted((i, j)))
        if key not in best:
            best[key] = (i, j, mscore, lex)
    return list(best.values())

def style_low_score_rows(df, threshold=0.75):
    def highlight(row):
        return ['background-color: #ffcccc' if (pd.notna(row['score']) and row['score'] < threshold) else '' for _ in row]
    return df.style.apply(highlight, axis=1)

# --------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Google Drive (–ø–æ ID) —á–µ—Ä–µ–∑ gdown
# --------------------

def download_file_from_gdrive(file_id: str) -> str:
    import gdown
    tmp_dir = tempfile.gettempdir()
    output_path = os.path.join(tmp_dir, f"model_gdrive_{file_id}")
    if not os.path.exists(output_path):
        url = f"https://drive.google.com/uc?id={file_id}"
        gdown.download(url, output_path, quiet=True)
    return output_path

@st.cache_resource(show_spinner=False)
def load_model_from_source(source: str, identifier: str) -> SentenceTransformer:
    """
    source: "huggingface" –∏–ª–∏ "gdrive"
    identifier: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ gdrive file_id
    """
    if source == "huggingface":
        model_path = identifier
    elif source == "gdrive":
        model_path = download_file_from_gdrive(identifier)
    else:
        raise ValueError("Unknown model source")
    model = SentenceTransformer(model_path)
    return model

def encode_texts_in_batches(model, texts: List[str], batch_size: int = 64) -> np.ndarray:
    if not texts:
        return np.array([])
    embs = model.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)
    return np.asarray(embs)

# --------------------
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Streamlit –∏ UI
# --------------------

st.set_page_config(page_title="Synonym Checker (with A/B, History)", layout="wide")
st.title("üîé Synonym Checker ‚Äî —Å –≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–∏, –∏—Å—Ç–æ—Ä–∏–µ–π, A/B —Ç–µ—Å—Ç–æ–º –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π")

# -- –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–æ–¥–µ–ª–∏ --

st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")

model_source = st.sidebar.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏", ["huggingface", "google_drive"], index=0)
if model_source == "huggingface":
    model_id = st.sidebar.text_input("Hugging Face Model ID", value="fine_tuned_model")
elif model_source == "google_drive":
    model_id = st.sidebar.text_input("Google Drive File ID", value="1RR15OMLj9vfSrVa1HN-dRU-4LbkdbRRf")

# –î–ª—è A/B —Ç–µ—Å—Ç–∞ (–¥–æ–ø. –º–æ–¥–µ–ª—å)
enable_ab_test = st.sidebar.checkbox("–í–∫–ª—é—á–∏—Ç—å A/B —Ç–µ—Å—Ç –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π", value=False)
if enable_ab_test:
    ab_model_source = st.sidebar.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏", ["huggingface", "google_drive"], index=0, key="ab_source")
    if ab_model_source == "huggingface":
        ab_model_id = st.sidebar.text_input("Hugging Face Model ID (B)", value="all-mpnet-base-v2", key="ab_id")
    elif ab_model_source == "google_drive":
        ab_model_id = st.sidebar.text_input("Google Drive File ID (B)", value="", key="ab_id")

batch_size = st.sidebar.number_input("Batch size –¥–ª—è —ç–Ω–∫–æ–¥–∏–Ω–≥–∞", min_value=8, max_value=1024, value=64, step=8)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
try:
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å..."):
        model_a = load_model_from_source(model_source, model_id)
    st.sidebar.success("–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
except Exception as e:
    st.sidebar.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
    st.stop()

model_b = None
if enable_ab_test:
    if ab_model_id.strip() == "":
        st.sidebar.warning("–í–≤–µ–¥–∏—Ç–µ ID –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è A/B —Ç–µ—Å—Ç–∞")
    else:
        try:
            with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å B..."):
                model_b = load_model_from_source(ab_model_source, ab_model_id)
            st.sidebar.success("–ú–æ–¥–µ–ª—å B –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            st.sidebar.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å B: {e}")
            st.stop()

# -- –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫ --
if "history" not in st.session_state:
    st.session_state["history"] = []

def add_to_history(record: dict):
    st.session_state["history"].append(record)

def clear_history():
    st.session_state["history"] = []

st.sidebar.header("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫")
if st.sidebar.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
    clear_history()

if st.sidebar.button("–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤ JSON"):
    if st.session_state["history"]:
        history_bytes = json.dumps(st.session_state["history"], indent=2, ensure_ascii=False).encode('utf-8')
        st.sidebar.download_button("–°–∫–∞—á–∞—Ç—å JSON", data=history_bytes, file_name="history.json", mime="application/json")
    else:
        st.sidebar.warning("–ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞—è")

# --------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –ø–∞—Ä–∞–º–∏
# --------------------

st.header("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: phrase_1, phrase_2, topics (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")

uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ —Ñ—Ä–∞–∑", type=["csv", "xlsx", "xls"])

if uploaded_file is not None:
    try:
        df, file_hash = read_uploaded_file_bytes(uploaded_file)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        st.stop()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_cols = {"phrase_1", "phrase_2"}
    if not required_cols.issubset(set(df.columns)):
        st.error(f"–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {required_cols}")
        st.stop()

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Å—Ç—Ä–æ–∫–∞–º
    df["phrase_1"] = df["phrase_1"].map(preprocess_text)
    df["phrase_2"] = df["phrase_2"].map(preprocess_text)
    if "topics" in df.columns:
        df["topics_list"] = df["topics"].map(parse_topics_field)
    else:
        df["topics_list"] = [[] for _ in range(len(df))]

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    phrases_all = list(set(df["phrase_1"].tolist() + df["phrase_2"].tolist()))
    phrase2idx = {p: i for i, p in enumerate(phrases_all)}

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—Ä–∞–∑ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å A
    with st.spinner("–ö–æ–¥–∏—Ä—É—é —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª—å—é A..."):
        embeddings_a = encode_texts_in_batches(model_a, phrases_all, batch_size)

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª—å—é B –¥–ª—è A/B —Ç–µ—Å—Ç–∞
    embeddings_b = None
    if enable_ab_test and model_b is not None:
        with st.spinner("–ö–æ–¥–∏—Ä—É—é —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª—å—é B..."):
            embeddings_b = encode_texts_in_batches(model_b, phrases_all, batch_size)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã
    scores = []
    scores_b = []
    lexical_scores = []
    for idx, row in df.iterrows():
        p1 = row["phrase_1"]
        p2 = row["phrase_2"]
        emb1_a = embeddings_a[phrase2idx[p1]]
        emb2_a = embeddings_a[phrase2idx[p2]]
        score_a = float(util.cos_sim(emb1_a, emb2_a).item())
        scores.append(score_a)

        # A/B
        if embeddings_b is not None:
            emb1_b = embeddings_b[phrase2idx[p1]]
            emb2_b = embeddings_b[phrase2idx[p2]]
            score_b = float(util.cos_sim(emb1_b, emb2_b).item())
            scores_b.append(score_b)

        # –õ–µ–∫—Å–∏—á–µ—Å–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞ (Jaccard —Ç–æ–∫–µ–Ω–æ–≤)
        lex_score = jaccard_tokens(p1, p2)
        lexical_scores.append(lex_score)

    df["score"] = scores
    if embeddings_b is not None:
        df["score_b"] = scores_b
    df["lexical_score"] = lexical_scores

    # –ü–æ–¥—Å–≤–µ—Ç–∫–∞ —Å—Ç—Ä–æ–∫ —Å –Ω–∏–∑–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, score < 0.75)
    highlight_threshold = st.slider("–ü–æ—Ä–æ–≥ –ø–æ–¥—Å–≤–µ—Ç–∫–∏ –Ω–∏–∑–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ (score)", min_value=0.0, max_value=1.0, value=0.75)

    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä")

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    result_csv = df.to_csv(index=False).encode('utf-8')
    st.download_button("–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CSV", data=result_csv, file_name="results.csv", mime="text/csv")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π
    styled_df = style_low_score_rows(df, threshold=highlight_threshold)
    st.dataframe(styled_df, use_container_width=True)

    # –û—Ç–¥–µ–ª—å–Ω–æ - –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è similarity score (–º–æ–¥–µ–ª—å A)")
    chart = alt.Chart(pd.DataFrame({"score": df["score"]})).mark_bar().encode(
        alt.X("score:Q", bin=alt.Bin(maxbins=30), title="Cosine similarity score"),
        y='count()', tooltip=['count()']
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

    if embeddings_b is not None:
        st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è similarity score (–º–æ–¥–µ–ª—å B)")
        chart_b = alt.Chart(pd.DataFrame({"score_b": df["score_b"]})).mark_bar().encode(
            alt.X("score_b:Q", bin=alt.Bin(maxbins=30), title="Cosine similarity score (B)"),
            y='count()', tooltip=['count()']
        ).interactive()
        st.altair_chart(chart_b, use_container_width=True)

    # –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä—É –≤ –∏—Å—Ç–æ—Ä–∏—é
    if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é"):
        record = {
            "file_hash": file_hash,
            "file_name": uploaded_file.name,
            "results": df.to_dict(orient="records"),
            "model_a": model_id,
            "model_b": ab_model_id if enable_ab_test else None,
            "timestamp": pd.Timestamp.now().isoformat(),
            "highlight_threshold": highlight_threshold,
        }
        add_to_history(record)
        st.success("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∏—Å—Ç–æ—Ä–∏—é.")

else:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏.")

# --- –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤–Ω–∏–∑—É ---
if st.session_state["history"]:
    st.header("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫")
    for idx, rec in enumerate(reversed(st.session_state["history"])):
        st.markdown(f"### –ü—Ä–æ–≤–µ—Ä–∫–∞ #{len(st.session_state['history']) - idx}")
        st.markdown(f"**–§–∞–π–ª:** {rec.get('file_name','-')}  |  **–î–∞—Ç–∞:** {rec.get('timestamp','-')}")
        st.markdown(f"**–ú–æ–¥–µ–ª—å A:** {rec.get('model_a','-')}  |  **–ú–æ–¥–µ–ª—å B:** {rec.get('model_b','-')}")
        saved_df = pd.DataFrame(rec["results"])
        styled_hist_df = style_low_score_rows(saved_df, rec.get("highlight_threshold", 0.75))
        st.dataframe(styled_hist_df, use_container_width=True)
        st.markdown("---")
