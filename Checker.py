import streamlit as st 
import pandas as pd
import numpy as np
import io
import hashlib
import json
import tempfile
import os
import shutil
import zipfile
import tarfile
from typing import List, Tuple, Dict, Any

from sentence_transformers import SentenceTransformer, util
import altair as alt

# --------------------
# –£—Ç–∏–ª–∏—Ç—ã
# --------------------

def preprocess_text(t: Any) -> str:
    if pd.isna(t):
        return ""
    return " ".join(str(t).lower().strip().split())

def file_md5(b: bytes) -> str:
    return hashlib.md5(b).hexdigest()

def read_uploaded_file_bytes(uploaded) -> Tuple[pd.DataFrame, str]:
    """–ü—Ä–æ—á–∏—Ç–∞—Ç—å CSV –∏–ª–∏ Excel –∏–∑ streamlit uploader –∏ –≤–µ—Ä–Ω—É—Ç—å DataFrame + md5 —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ."""
    raw = uploaded.read()
    h = file_md5(raw)
    try:
        df = pd.read_csv(io.BytesIO(raw))
    except Exception:
        try:
            df = pd.read_excel(io.BytesIO(raw))
        except Exception as e:
            raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å CSV –∏–ª–∏ Excel. –û—à–∏–±–∫–∞: " + str(e))
    return df, h

def parse_topics_field(val) -> List[str]:
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø–æ–ª–µ topics –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫."""
    if pd.isna(val):
        return []
    if isinstance(val, list):
        return [str(x).strip() for x in val if str(x).strip()]
    s = str(val).strip()
    if (s.startswith("[") and s.endswith("]")) or (s.startswith("(") and s.endswith(")")):
        try:
            parsed = json.loads(s)
            if isinstance(parsed, list):
                return [str(x).strip() for x in parsed if str(x).strip()]
        except Exception:
            pass
    for sep in [";", "|", ","]:
        if sep in s:
            return [p.strip() for p in s.split(sep) if p.strip()]
    return [s] if s else []

def jaccard_tokens(a: str, b: str) -> float:
    sa = set([t for t in a.split() if t])
    sb = set([t for t in b.split() if t])
    if not sa and not sb:
        return 0.0
    inter = sa & sb
    union = sa | sb
    return len(inter) / len(union) if union else 0.0

def style_low_score_rows(df, threshold=0.75):
    def highlight(row):
        score_val = row.get('score', None)
        try:
            cond = pd.notna(score_val) and float(score_val) < threshold
        except Exception:
            cond = False
        return ['background-color: #ffcccc' if cond else '' for _ in row]
    return df.style.apply(highlight, axis=1)

def style_suspicious_and_low(df, sem_thresh: float, lex_thresh: float, low_score_thresh: float):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç styled dataframe: 
    - —Å—Ç—Ä–æ–∫–∏ —Å score < low_score_thresh -> —Ä–æ–∑–æ–≤—ã–π (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
    - —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è '–Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã–º–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏' (score >= sem_thresh and lexical_score <= lex_thresh) -> –∂—ë–ª—Ç–æ-–∑–µ–ª—ë–Ω—ã–π
    –ï—Å–ª–∏ –æ–±–µ –º–µ—Ç–∫–∏ ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç '–Ω–µ–æ—á–µ–≤–∏–¥–Ω–æ–≥–æ' (–æ—Ç–ª–∏—á–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫).
    """
    def highlight(row):
        out = []
        try:
            score = float(row.get('score', 0))
        except Exception:
            score = 0.0
        try:
            lex = float(row.get('lexical_score', 0))
        except Exception:
            lex = 0.0

        is_low_score = (score < low_score_thresh)
        is_suspicious = (score >= sem_thresh and lex <= lex_thresh)
        for _ in row:
            if is_suspicious:
                out.append('background-color: #fff2b8')  # light yellow/orange for suspicious semantic matches
            elif is_low_score:
                out.append('background-color: #ffcccc')  # light red for low semantic score
            else:
                out.append('')
        return out
    return df.style.apply(highlight, axis=1)

# --------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Google Drive —Å —Ä–∞—Å–ø–∞–∫–æ–≤–∫–æ–π
# --------------------

def download_file_from_gdrive(file_id: str) -> str:
    import gdown
    tmp_dir = tempfile.gettempdir()
    archive_path = os.path.join(tmp_dir, f"model_gdrive_{file_id}")
    model_dir = os.path.join(tmp_dir, f"model_gdrive_extracted_{file_id}")

    if not os.path.exists(archive_path):
        url = f"https://drive.google.com/uc?id={file_id}"
        gdown.download(url, archive_path, quiet=True)

    if os.path.exists(model_dir) and os.path.isdir(model_dir):
        return model_dir

    os.makedirs(model_dir, exist_ok=True)
    if zipfile.is_zipfile(archive_path):
        with zipfile.ZipFile(archive_path, 'r') as zip_ref:
            zip_ref.extractall(model_dir)
    elif tarfile.is_tarfile(archive_path):
        with tarfile.open(archive_path, 'r:*') as tar_ref:
            tar_ref.extractall(model_dir)
    else:
        try:
            shutil.copy(archive_path, model_dir)
        except Exception:
            pass

    return model_dir

@st.cache_resource(show_spinner=False)
def load_model_from_source(source: str, identifier: str) -> SentenceTransformer:
    if source == "huggingface":
        model_path = identifier
    elif source == "google_drive":
        model_path = download_file_from_gdrive(identifier)
    else:
        raise ValueError("Unknown model source")
    model = SentenceTransformer(model_path)
    return model

def encode_texts_in_batches(model, texts: List[str], batch_size: int = 64) -> np.ndarray:
    if not texts:
        return np.array([])
    embs = model.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=False)
    return np.asarray(embs)

# --------------------
# UI
# --------------------

st.set_page_config(page_title="Synonym Checker (with A/B, History)", layout="wide")
st.title("üîé Synonym Checker")

# -- –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–æ–¥–µ–ª–∏ --
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏")

model_source = st.sidebar.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –º–æ–¥–µ–ª–∏", ["huggingface", "google_drive"], index=0)
if model_source == "huggingface":
    model_id = st.sidebar.text_input("Hugging Face Model ID", value="fine_tuned_model")
elif model_source == "google_drive":
    model_id = st.sidebar.text_input("Google Drive File ID", value="1RR15OMLj9vfSrVa1HN-dRU-4LbkdbRRf")

enable_ab_test = st.sidebar.checkbox("–í–∫–ª—é—á–∏—Ç—å A/B —Ç–µ—Å—Ç –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π", value=False)
if enable_ab_test:
    ab_model_source = st.sidebar.selectbox("–ò—Å—Ç–æ—á–Ω–∏–∫ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏", ["huggingface", "google_drive"], index=0, key="ab_source")
    if ab_model_source == "huggingface":
        ab_model_id = st.sidebar.text_input("Hugging Face Model ID (B)", value="all-mpnet-base-v2", key="ab_id")
    elif ab_model_source == "google_drive":
        ab_model_id = st.sidebar.text_input("Google Drive File ID (B)", value="", key="ab_id")
else:
    ab_model_id = ""

batch_size = st.sidebar.number_input("Batch size –¥–ª—è —ç–Ω–∫–æ–¥–∏–Ω–≥–∞", min_value=8, max_value=1024, value=64, step=8)

# ---- Detector settings for semantic-high / lexical-low
st.sidebar.header("–î–µ—Ç–µ–∫—Ç–æ—Ä –Ω–µ–æ—á–µ–≤–∏–¥–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
enable_detector = st.sidebar.checkbox("–í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–µ–∫—Ç–æ—Ä (high sem, low lex)", value=True)
semantic_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ (>=)", 0.0, 1.0, 0.80, 0.01)
lexical_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–π –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (<=)", 0.0, 1.0, 0.30, 0.01)
low_score_threshold = st.sidebar.slider("–ü–æ—Ä–æ–≥ –Ω–∏–∑–∫–æ–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ (–Ω–∏–∑–∫–∞—è –ø–æ–¥—Å–≤–µ—Ç–∫–∞)", 0.0, 1.0, 0.75, 0.01)

try:
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å..."):
        model_a = load_model_from_source(model_source, model_id)
    st.sidebar.success("–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
except Exception as e:
    st.sidebar.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
    st.stop()

model_b = None
if enable_ab_test:
    if ab_model_id.strip() == "":
        st.sidebar.warning("–í–≤–µ–¥–∏—Ç–µ ID –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è A/B —Ç–µ—Å—Ç–∞")
    else:
        try:
            with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å B..."):
                model_b = load_model_from_source(ab_model_source, ab_model_id)
            st.sidebar.success("–ú–æ–¥–µ–ª—å B –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            st.sidebar.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å B: {e}")
            st.stop()

# –ò—Å—Ç–æ—Ä–∏—è
if "history" not in st.session_state:
    st.session_state["history"] = []

# Suggestions store for autocomplete
if "suggestions" not in st.session_state:
    st.session_state["suggestions"] = []  # list of phrases (strings)

def add_to_history(record: dict):
    st.session_state["history"].append(record)

def clear_history():
    st.session_state["history"] = []

def add_suggestions(phrases: List[str]):
    """–î–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ –≤ suggestions (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ, –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–≤–µ—Ä—Ö—É)."""
    s = [p for p in phrases if p and isinstance(p, str)]
    for p in reversed(s):
        if p not in st.session_state["suggestions"]:
            st.session_state["suggestions"].insert(0, p)
    st.session_state["suggestions"] = st.session_state["suggestions"][:200]

# Helper to set manual input values via button callbacks
def _set_manual_value(key: str, val: str):
    st.session_state[key] = val

st.sidebar.header("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫")
if st.sidebar.button("–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é"):
    clear_history()

if st.sidebar.button("–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤ JSON"):
    if st.session_state["history"]:
        history_bytes = json.dumps(st.session_state["history"], indent=2, ensure_ascii=False).encode('utf-8')
        st.sidebar.download_button("–°–∫–∞—á–∞—Ç—å JSON", data=history_bytes, file_name="history.json", mime="application/json")
    else:
        st.sidebar.warning("–ò—Å—Ç–æ—Ä–∏—è –ø—É—Å—Ç–∞—è")

# --------------------
# –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: —Ñ–∞–π–ª –∏–ª–∏ —Ä—É—á–Ω–æ–π –≤–≤–æ–¥
# --------------------
mode = st.radio("–†–µ–∂–∏–º –ø—Ä–æ–≤–µ—Ä–∫–∏", ["–§–∞–π–ª (CSV/XLSX)", "–†—É—á–Ω–æ–π –≤–≤–æ–¥"], index=0, horizontal=True)

# --------------------
# –†—É—á–Ω–æ–π –≤–≤–æ–¥: –æ–¥–Ω–∞ –ø–∞—Ä–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ
# --------------------
if mode == "–†—É—á–Ω–æ–π –≤–≤–æ–¥":
    st.header("–†—É—á–Ω–æ–π –≤–≤–æ–¥ –ø–∞—Ä —Ñ—Ä–∞–∑")

    # Show top suggestions if any
    if st.session_state["suggestions"]:
        st.caption("–ü–æ–¥—Å–∫–∞–∑–∫–∏ (–Ω–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã –≤—Å—Ç–∞–≤–∏—Ç—å –≤ –ø–æ–ª–µ):")
        cols = st.columns(5)
        for i, s_phrase in enumerate(st.session_state["suggestions"][:20]):
            col = cols[i % 5]
            if col.button(s_phrase, key=f"sugg_{i}"):
                if not st.session_state.get("manual_text1"):
                    st.session_state["manual_text1"] = s_phrase
                else:
                    st.session_state["manual_text2"] = s_phrase

    # Single pair with autocomplete helper buttons below inputs
    with st.expander("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ–¥–Ω—É –ø–∞—Ä—É —Ñ—Ä–∞–∑ (–±—ã—Å—Ç—Ä–æ)"):
        if "manual_text1" not in st.session_state:
            st.session_state["manual_text1"] = ""
        if "manual_text2" not in st.session_state:
            st.session_state["manual_text2"] = ""

        text1 = st.text_input("–§—Ä–∞–∑–∞ 1", key="manual_text1")
        if st.session_state["suggestions"]:
            s_cols = st.columns(10)
            for i, sp in enumerate(st.session_state["suggestions"][:10]):
                if s_cols[i % 10].button(sp, key=f"t1_sugg_{i}"):
                    pass

        text2 = st.text_input("–§—Ä–∞–∑–∞ 2", key="manual_text2")
        if st.session_state["suggestions"]:
            s_cols2 = st.columns(10)
            for i, sp in enumerate(st.session_state["suggestions"][:10]):
                if s_cols2[i % 10].button(sp, key=f"t2_sugg_{i}"):
                    pass

        if st.button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞—Ä—É", key="manual_check"):
            if not text1 or not text2:
                st.warning("–í–≤–µ–¥–∏—Ç–µ –æ–±–µ —Ñ—Ä–∞–∑—ã.")
            else:
                t1 = preprocess_text(text1)
                t2 = preprocess_text(text2)
                add_suggestions([t1, t2])

                emb1 = encode_texts_in_batches(model_a, [t1], batch_size)
                emb2 = encode_texts_in_batches(model_a, [t2], batch_size)
                score_a = float(util.cos_sim(emb1[0], emb2[0]).item())
                lex = jaccard_tokens(t1, t2)

                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç (–º–æ–¥–µ–ª—å A)")
                col1, col2, col3 = st.columns([1,1,1])
                col1.metric("Score A", f"{score_a:.4f}")
                col2.metric("Jaccard (lexical)", f"{lex:.4f}")

                # Check detector for single pair
                is_suspicious_single = False
                if enable_detector and (score_a >= semantic_threshold) and (lex <= lexical_threshold):
                    is_suspicious_single = True
                    st.warning("–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ù–ï–û–ß–ï–í–ò–î–ù–û–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: –≤—ã—Å–æ–∫–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å, –Ω–∏–∑–∫–∞—è –ª–µ–∫—Å–∏—á–µ—Å–∫–∞—è –ø–æ—Ö–æ–∂–µ—Å—Ç—å.")

                if model_b is not None:
                    emb1b = encode_texts_in_batches(model_b, [t1], batch_size)
                    emb2b = encode_texts_in_batches(model_b, [t2], batch_size)
                    score_b = float(util.cos_sim(emb1b[0], emb2b[0]).item())
                    delta = score_b - score_a
                    col3.metric("Score B", f"{score_b:.4f}", delta=f"{delta:+.4f}")
                    comp_df = pd.DataFrame({
                        "model": ["A", "B"],
                        "score": [score_a, score_b]
                    })
                    chart = alt.Chart(comp_df).mark_bar().encode(
                        x=alt.X('model:N', title=None),
                        y=alt.Y('score:Q', scale=alt.Scale(domain=[0,1]), title="Cosine similarity score"),
                        tooltip=['model','score']
                    )
                    st.altair_chart(chart.properties(width=300), use_container_width=False)
                else:
                    col3.write("")

                if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é", key="save_manual_single"):
                    rec = {
                        "source": "manual_single",
                        "pair": {"phrase_1": t1, "phrase_2": t2},
                        "score": score_a,
                        "score_b": float(score_b) if model_b is not None else None,
                        "lexical_score": lex,
                        "is_suspicious": is_suspicious_single,
                        "model_a": model_id,
                        "model_b": ab_model_id if enable_ab_test else None,
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    add_to_history(rec)
                    st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏.")

    # Bulk manual: textarea, one pair per line
    with st.expander("–í–≤–µ—Å—Ç–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä (–∫–∞–∂–¥–∞—è –ø–∞—Ä–∞ –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ). –§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏: `—Ñ—Ä–∞–∑–∞1 || —Ñ—Ä–∞–∑–∞2` –∏–ª–∏ `—Ñ—Ä–∞–∑–∞1<TAB>—Ñ—Ä–∞–∑–∞2` –∏–ª–∏ `—Ñ—Ä–∞–∑–∞1,—Ñ—Ä–∞–∑–∞2`"):
        bulk_text = st.text_area("–í—Å—Ç–∞–≤—å—Ç–µ –ø–∞—Ä—ã (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ)", height=180, key="bulk_pairs")
        st.caption("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏: `||`, —Ç–∞–±, `,`. –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `||`.")
        if st.button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—Å–µ –ø–∞—Ä—ã (—Ä—É—á–Ω–æ–π –≤–≤–æ–¥)", key="manual_bulk_check"):
            lines = [l.strip() for l in bulk_text.splitlines() if l.strip()]
            if not lines:
                st.warning("–ù–∏—á–µ–≥–æ –Ω–µ –≤–≤–µ–¥–µ–Ω–æ.")
            else:
                parsed = []
                for ln in lines:
                    if "||" in ln:
                        p1, p2 = ln.split("||", 1)
                    elif "\t" in ln:
                        p1, p2 = ln.split("\t", 1)
                    elif "," in ln:
                        p1, p2 = ln.split(",", 1)
                    else:
                        p1, p2 = ln, ""
                    parsed.append((preprocess_text(p1), preprocess_text(p2)))
                parsed = [(a,b) for a,b in parsed if a and b]
                if not parsed:
                    st.warning("–ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –ø–∞—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç).")
                else:
                    add_suggestions([p for pair in parsed for p in pair])

                    phrases_all = list({p for pair in parsed for p in pair})
                    phrase2idx = {p: i for i, p in enumerate(phrases_all)}
                    with st.spinner("–ö–æ–¥–∏—Ä—É—é —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª—å—é A..."):
                        embeddings_a = encode_texts_in_batches(model_a, phrases_all, batch_size)
                    embeddings_b = None
                    if model_b is not None:
                        with st.spinner("–ö–æ–¥–∏—Ä—É—é —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª—å—é B..."):
                            embeddings_b = encode_texts_in_batches(model_b, phrases_all, batch_size)
                    rows = []
                    for p1, p2 in parsed:
                        emb1 = embeddings_a[phrase2idx[p1]]
                        emb2 = embeddings_a[phrase2idx[p2]]
                        score_a = float(util.cos_sim(emb1, emb2).item())
                        score_b = None
                        if embeddings_b is not None:
                            emb1b = embeddings_b[phrase2idx[p1]]
                            emb2b = embeddings_b[phrase2idx[p2]]
                            score_b = float(util.cos_sim(emb1b, emb2b).item())
                        lex = jaccard_tokens(p1, p2)
                        rows.append({"phrase_1": p1, "phrase_2": p2, "score": score_a, "score_b": score_b, "lexical_score": lex})
                    res_df = pd.DataFrame(rows)
                    st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Ä—É—á–Ω–æ–π –º–∞—Å—Å–æ–≤—ã–π –≤–≤–æ–¥)")
                    # styled with both types of highlights
                    styled = style_suspicious_and_low(res_df, semantic_threshold, lexical_threshold, low_score_threshold)
                    st.dataframe(styled, use_container_width=True)
                    csv_bytes = res_df.to_csv(index=False).encode('utf-8')
                    st.download_button("–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CSV", data=csv_bytes, file_name="manual_results.csv", mime="text/csv")

                    # Suspicious subset
                    if enable_detector:
                        susp_df = res_df[(res_df["score"] >= semantic_threshold) & (res_df["lexical_score"] <= lexical_threshold)]
                        if not susp_df.empty:
                            st.markdown("### –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (high semantic, low lexical)")
                            st.write(f"–ù–∞–π–¥–µ–Ω–æ {len(susp_df)} –ø–∞—Ä.")
                            st.dataframe(susp_df, use_container_width=True)
                            susp_csv = susp_df.to_csv(index=False).encode('utf-8')
                            st.download_button("–°–∫–∞—á–∞—Ç—å suspicious CSV", data=susp_csv, file_name="suspicious_manual_bulk.csv", mime="text/csv")
                            if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å suspicious –≤ –∏—Å—Ç–æ—Ä–∏—é", key="save_susp_manual"):
                                rec = {
                                    "source": "manual_bulk_suspicious",
                                    "pairs_count": len(susp_df),
                                    "results": susp_df.to_dict(orient="records"),
                                    "model_a": model_id,
                                    "model_b": ab_model_id if enable_ab_test else None,
                                    "timestamp": pd.Timestamp.now().isoformat(),
                                    "semantic_threshold": semantic_threshold,
                                    "lexical_threshold": lexical_threshold
                                }
                                add_to_history(rec)
                                st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏.")

# --------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –ø–∞—Ä–∞–º–∏ (—Å—Ç–∞—Ä—ã–π —Ä–µ–∂–∏–º)
# --------------------
if mode == "–§–∞–π–ª (CSV/XLSX)":
    st.header("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV –∏–ª–∏ Excel —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: phrase_1, phrase_2, topics (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")

    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ —Ñ—Ä–∞–∑", type=["csv", "xlsx", "xls"])

    if uploaded_file is not None:
        try:
            df, file_hash = read_uploaded_file_bytes(uploaded_file)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            st.stop()

        required_cols = {"phrase_1", "phrase_2"}
        if not required_cols.issubset(set(df.columns)):
            st.error(f"–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {required_cols}")
            st.stop()

        df["phrase_1"] = df["phrase_1"].map(preprocess_text)
        df["phrase_2"] = df["phrase_2"].map(preprocess_text)
        if "topics" in df.columns:
            df["topics_list"] = df["topics"].map(parse_topics_field)
        else:
            df["topics_list"] = [[] for _ in range(len(df))]

        # add file phrases to suggestions for autocomplete
        add_suggestions(list(set(df["phrase_1"].tolist() + df["phrase_2"].tolist())))

        phrases_all = list(set(df["phrase_1"].tolist() + df["phrase_2"].tolist()))
        phrase2idx = {p: i for i, p in enumerate(phrases_all)}

        with st.spinner("–ö–æ–¥–∏—Ä—É—é —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª—å—é A..."):
            embeddings_a = encode_texts_in_batches(model_a, phrases_all, batch_size)

        embeddings_b = None
        if enable_ab_test and model_b is not None:
            with st.spinner("–ö–æ–¥–∏—Ä—É—é —Ñ—Ä–∞–∑—ã –º–æ–¥–µ–ª—å—é B..."):
                embeddings_b = encode_texts_in_batches(model_b, phrases_all, batch_size)

        scores = []
        scores_b = []
        lexical_scores = []
        for _, row in df.iterrows():
            p1 = row["phrase_1"]
            p2 = row["phrase_2"]
            emb1_a = embeddings_a[phrase2idx[p1]]
            emb2_a = embeddings_a[phrase2idx[p2]]
            score_a = float(util.cos_sim(emb1_a, emb2_a).item())
            scores.append(score_a)

            if embeddings_b is not None:
                emb1_b = embeddings_b[phrase2idx[p1]]
                emb2_b = embeddings_b[phrase2idx[p2]]
                score_b = float(util.cos_sim(emb1_b, emb2_b).item())
                scores_b.append(score_b)

            lex_score = jaccard_tokens(p1, p2)
            lexical_scores.append(lex_score)

        df["score"] = scores
        if embeddings_b is not None:
            df["score_b"] = scores_b
        df["lexical_score"] = lexical_scores

        highlight_threshold = low_score_threshold  # use chosen threshold

        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä")
        result_csv = df.to_csv(index=False).encode('utf-8')
        st.download_button("–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CSV", data=result_csv, file_name="results.csv", mime="text/csv")

        # styled with both types of highlights
        styled_df = style_suspicious_and_low(df, semantic_threshold, lexical_threshold, low_score_threshold)
        st.dataframe(styled_df, use_container_width=True)

        st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è similarity score (–º–æ–¥–µ–ª—å A)")
        chart = alt.Chart(pd.DataFrame({"score": df["score"]})).mark_bar().encode(
            alt.X("score:Q", bin=alt.Bin(maxbins=30), title="Cosine similarity score"),
            y='count()', tooltip=['count()']
        ).interactive()
        st.altair_chart(chart, use_container_width=True)

        if embeddings_b is not None:
            st.subheader("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è similarity score (–º–æ–¥–µ–ª—å B)")
            chart_b = alt.Chart(pd.DataFrame({"score_b": df["score_b"]})).mark_bar().encode(
                alt.X("score_b:Q", bin=alt.Bin(maxbins=30), title="Cosine similarity score (B)"),
                y='count()', tooltip=['count()']
            ).interactive()
            st.altair_chart(chart_b, use_container_width=True)

        # Suspicious subset for file mode
        if enable_detector:
            susp_df = df[(df["score"] >= semantic_threshold) & (df["lexical_score"] <= lexical_threshold)]
            st.markdown("### –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (high semantic, low lexical)")
            if susp_df.empty:
                st.write("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–∞–º.")
            else:
                st.write(f"–ù–∞–π–¥–µ–Ω–æ {len(susp_df)} –ø–∞—Ä.")
                st.dataframe(susp_df, use_container_width=True)
                susp_csv = susp_df.to_csv(index=False).encode('utf-8')
                st.download_button("–°–∫–∞—á–∞—Ç—å suspicious CSV", data=susp_csv, file_name="suspicious_file_mode.csv", mime="text/csv")
                if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å suspicious –≤ –∏—Å—Ç–æ—Ä–∏—é (file)", key="save_susp_file"):
                    rec = {
                        "source": "file_suspicious",
                        "file_hash": file_hash,
                        "file_name": uploaded_file.name,
                        "pairs_count": len(susp_df),
                        "results": susp_df.to_dict(orient="records"),
                        "model_a": model_id,
                        "model_b": ab_model_id if enable_ab_test else None,
                        "timestamp": pd.Timestamp.now().isoformat(),
                        "semantic_threshold": semantic_threshold,
                        "lexical_threshold": lexical_threshold
                    }
                    add_to_history(rec)
                    st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏.")

        if st.button("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ –∏—Å—Ç–æ—Ä–∏—é"):
            record = {
                "file_hash": file_hash,
                "file_name": uploaded_file.name,
                "results": df.to_dict(orient="records"),
                "model_a": model_id,
                "model_b": ab_model_id if enable_ab_test else None,
                "timestamp": pd.Timestamp.now().isoformat(),
                "highlight_threshold": highlight_threshold,
            }
            add_to_history(record)
            st.success("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∏—Å—Ç–æ—Ä–∏—é.")

    else:
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏.")

# --- –ü–æ–∫–∞–∑–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –≤–Ω–∏–∑—É ---
if st.session_state["history"]:
    st.header("–ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ–≤–µ—Ä–æ–∫")
    for idx, rec in enumerate(reversed(st.session_state["history"])):
        st.markdown(f"### –ü—Ä–æ–≤–µ—Ä–∫–∞ #{len(st.session_state['history']) - idx}")
        # –í –∏—Å—Ç–æ—Ä–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞–ø–∏—Å–∏ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ ‚Äî —Ä—É—á–Ω–æ–π single, manual_bulk, file, suspicious entries
        if rec.get("source") == "manual_single":
            p = rec.get("pair", {})
            st.markdown(f"**–†—É—á–Ω–æ–π –≤–≤–æ–¥ (single)**  |  **–î–∞—Ç–∞:** {rec.get('timestamp','-')}")
            st.markdown(f"**–§—Ä–∞–∑—ã:** `{p.get('phrase_1','')}`  ‚Äî `{p.get('phrase_2','')}`")
            st.markdown(f"**Score A:** {rec.get('score', '-')}, **Score B:** {rec.get('score_b', '-')}, **Lexical:** {rec.get('lexical_score','-')}")
            if rec.get("is_suspicious"):
                st.warning("–≠—Ç–∞ –ø–∞—Ä–∞ –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ –Ω–µ–æ—á–µ–≤–∏–¥–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (high semantic, low lexical).")
        elif rec.get("source") == "manual_bulk":
            st.markdown(f"**–†—É—á–Ω–æ–π –≤–≤–æ–¥ (bulk)**  |  **–î–∞—Ç–∞:** {rec.get('timestamp','-')}")
            st.markdown(f"**–ü–∞—Ä:** {rec.get('pairs_count', 0)}  |  **–ú–æ–¥–µ–ª—å A:** {rec.get('model_a','-')}")
            saved_df = pd.DataFrame(rec.get("results", []))
            if not saved_df.empty:
                styled_hist_df = style_suspicious_and_low(saved_df, rec.get("semantic_threshold", semantic_threshold), rec.get("lexical_threshold", lexical_threshold), low_score_threshold)
                st.dataframe(styled_hist_df, use_container_width=True)
        elif rec.get("source") in ("manual_bulk_suspicious", "manual_bulk_suspicious"):
            st.markdown(f"**–†—É—á–Ω–æ–π suspicious**  |  **–î–∞—Ç–∞:** {rec.get('timestamp','-')}")
            st.markdown(f"**–ü–∞—Ä:** {rec.get('pairs_count', 0)}  |  **–ú–æ–¥–µ–ª—å A:** {rec.get('model_a','-')}")
            saved_df = pd.DataFrame(rec.get("results", []))
            if not saved_df.empty:
                st.dataframe(saved_df, use_container_width=True)
        elif rec.get("source") == "file_suspicious":
            st.markdown(f"**–§–∞–π–ª (suspicious)**  |  **–§–∞–π–ª:** {rec.get('file_name','-')}  |  **–î–∞—Ç–∞:** {rec.get('timestamp','-')}")
            st.markdown(f"**–ü–∞—Ä:** {rec.get('pairs_count', 0)}  |  **–ú–æ–¥–µ–ª—å A:** {rec.get('model_a','-')}")
            saved_df = pd.DataFrame(rec.get("results", []))
            if not saved_df.empty:
                st.dataframe(saved_df, use_container_width=True)
        else:
            st.markdown(f"**–§–∞–π–ª:** {rec.get('file_name','-')}  |  **–î–∞—Ç–∞:** {rec.get('timestamp','-')}")
            st.markdown(f"**–ú–æ–¥–µ–ª—å A:** {rec.get('model_a','-')}  |  **–ú–æ–¥–µ–ª—å B:** {rec.get('model_b','-')}")
            saved_df = pd.DataFrame(rec.get("results", []))
            if not saved_df.empty:
                styled_hist_df = style_suspicious_and_low(saved_df, rec.get("semantic_threshold", semantic_threshold), rec.get("lexical_threshold", lexical_threshold), low_score_threshold)
                st.dataframe(styled_hist_df, use_container_width=True)
        st.markdown("---")
